{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "An implementation for training a DQN agent to play atari games. Doesn't quite solve the games yet; \n",
    "it needs a bit more fine tuning. Some training tricks (experience replay, target networking etc.) \n",
    "are probably a good place to start.\n",
    "\n",
    "Author: Noorvir Aulakh\n",
    "Date: 15/03/2017\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import csv\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "\n",
    "import gym\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.viewer import ImageViewer\n",
    "from skimage.transform import resize\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import img_as_ubyte\n",
    "from skimage import exposure\n",
    "import warnings\n",
    "\n",
    "games = ['Pong-v3', 'MsPacman-v3', 'Boxing-v3']\n",
    "\n",
    "# ==================================================================================================\n",
    "# Parameters\n",
    "\n",
    "isTRAIN = True\n",
    "# Problem subsection to run (1, 2 or 3)\n",
    "# Part 1 - Report score and frame counts under a random policy, and average mean and stddev\n",
    "# Part 2 -\n",
    "# Part 3 -\n",
    "PART_NUM = 2\n",
    "GAME_NUM = 0  # Game to play: 1 - Pong, 2 - Pacman, 3 - Boxing\n",
    "\n",
    "EX_BUFFER_SIZE = 100000  # Size of experience replay buffer\n",
    "EPSILON = 0.1  # Exploration parameter\n",
    "GAMMA = 0.99  # Discount factor\n",
    "NUM_ITER = 10**6               # Number of epochs to run for\n",
    "LEARNING_RATE = 10 ** -3  # Step-size for optimiser\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "TRAINING_MOVING_AVG_ARR_LEN = 5000\n",
    "# ==================================================================================================\n",
    "\n",
    "# ==================================================================================================\n",
    "# Save Options\n",
    "LOG_FOLDER = './logs/part_b/'\n",
    "MODEL_FOLDER = './models/part_b/'\n",
    "LOAD_MODEL_FILENAME = ''\n",
    "# ==================================================================================================\n",
    "\n",
    "game_env = gym.make(games[GAME_NUM])\n",
    "\n",
    "print(game_env.action_space.n)\n",
    "print(game_env.observation_space)\n",
    "\n",
    "\n",
    "class AtariSan:\n",
    "    def __init__(self, env=game_env, experience_buff_size=EX_BUFFER_SIZE, epsilon=EPSILON,\n",
    "                 gamma=GAMMA, num_iter=NUM_ITER, learning_rate=LEARNING_RATE,\n",
    "                 batch_size=BATCH_SIZE):\n",
    "        \"\"\"\n",
    "\n",
    "        :param env:\n",
    "        :param experience_buff_size:\n",
    "        :param epsilon:\n",
    "        :param gamma:\n",
    "        :param num_iter:\n",
    "        :param learning_rate:\n",
    "        :param batch_size:\n",
    "        \"\"\"\n",
    "\n",
    "        self.env = env\n",
    "        self.experience_buff_size = experience_buff_size\n",
    "        self.experience_buff = deque(maxlen=self.experience_buff_size)\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.num_iter = num_iter\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.action_space = env.action_space.n\n",
    "        self.state = []\n",
    "        self.next_state = []\n",
    "\n",
    "        # Initialise graphs\n",
    "        with tf.variable_scope('nn_main'):\n",
    "            self.X_main, weights_main, self.nn_main = self.build_graph()\n",
    "            self.W_conv1, self.b_conv1, self.W_conv2, self.b_conv2, self.W_fcl, self.b_fcl, \\\n",
    "                self.W_lin, self.b_lin = weights_main\n",
    "\n",
    "        with tf.variable_scope('nn_target'):\n",
    "            self.X_target, weights_target, self.nn_target = self.build_graph()\n",
    "            self.W_Tconv1, self.b_Tconv1, self.W_Tconv2, self.b_Tconv2, self.W_Tfcl, self.b_Tfcl, \\\n",
    "                self.W_Tlin, self.b_Tlin = weights_target\n",
    "\n",
    "        # Define operation to transfer weights from main network to target network\n",
    "        self.update_target_net = [\n",
    "            self.W_Tconv1.assign(self.W_conv1), self.b_Tconv1.assign(self.b_conv1),\n",
    "            self.W_Tconv2.assign(self.W_conv2), self.b_Tconv2.assign(self.b_conv2),\n",
    "            self.W_Tfcl.assign(self.W_fcl), self.b_Tfcl.assign(self.b_fcl),\n",
    "            self.W_Tlin.assign(self.W_lin), self.b_Tlin.assign(self.b_lin)]\n",
    "\n",
    "    def pre_process(self, observation):\n",
    "        \"\"\"\n",
    "        Preprocess frames by resizing them to 28x28 and stacking four frames together to provide\n",
    "        temporal information\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "\n",
    "            observation = resize(rgb2gray(observation)[35:195, :], (28, 28))\n",
    "            observation = exposure.rescale_intensity(observation)\n",
    "            observation = img_as_ubyte(observation)\n",
    "\n",
    "            # ImageViewer(observation).show()\n",
    "\n",
    "        return observation\n",
    "\n",
    "    def build_graph(self):\n",
    "        \"\"\"\n",
    "        For the first layer use a filter size of 6x6 with a stride of 2 and 16 channels followed by\n",
    "        a ReLU. For the second layer use a filter size of 4x4 with a stride of 2 and 32 channels\n",
    "        followed by a ReLU. Flatten the output and add a fully connected third layer with 256 units\n",
    "        followed by a ReLU. Finally, we have a linear layer that predicts the state-action value\n",
    "        function with one output for each action.\n",
    "        :param part_num:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        X = tf.placeholder(tf.float32, [None, 28, 28, 4], name='X')\n",
    "\n",
    "        # Initialise weights\n",
    "        W_conv1 = tf.Variable(tf.truncated_normal([6, 6, 4, 16], stddev=0.01), name='W_conv1')\n",
    "        b_conv1 = tf.Variable(tf.truncated_normal([16]), name='b_conv1')\n",
    "\n",
    "        W_conv2 = tf.Variable(tf.random_normal([4, 4, 16, 32], stddev=0.01), name='W_conv2')\n",
    "        b_conv2 = tf.Variable(tf.truncated_normal([32]), name='b_conv2')\n",
    "\n",
    "        W_fcl = tf.Variable(tf.truncated_normal([7 * 7 * 32, 256]), name='W_fcl')\n",
    "        b_fcl = tf.Variable(tf.truncated_normal([256]), name='b_fcl')\n",
    "\n",
    "        W_lin = tf.Variable(tf.truncated_normal([256, self.action_space], stddev=0.01),\n",
    "                            name='W_lin')\n",
    "        b_lin = tf.Variable(tf.truncated_normal([self.action_space], stddev=0.01), name='b_lin')\n",
    "\n",
    "        # Convolution layers\n",
    "        conv_l1 = tf.nn.relu(tf.nn.conv2d(X, W_conv1, strides=[1, 2, 2, 1],\n",
    "                                          padding='SAME') + b_conv1, name='conv_1l')\n",
    "\n",
    "        conv_21 = tf.nn.relu(tf.nn.conv2d(conv_l1, W_conv2, strides=[1, 2, 2, 1],\n",
    "                                          padding='SAME') + b_conv2, name='conv_2l')\n",
    "\n",
    "        # Fully connected layer\n",
    "        fcl_input = tf.reshape(conv_21, [-1, 7 * 7 * 32])\n",
    "        fcl = tf.nn.relu(tf.matmul(fcl_input, W_fcl) + b_fcl, name='fcl')\n",
    "\n",
    "        # Linear Layer\n",
    "        output_l = tf.matmul(fcl, W_lin) + b_lin\n",
    "\n",
    "        weights = [W_conv1, b_conv1, W_conv2, b_conv2, W_fcl, b_fcl, W_lin, b_lin]\n",
    "\n",
    "        return X, weights, output_l\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train DQN.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # Define training-specific placeholders\n",
    "        Q_target = tf.placeholder(tf.float32, [None, 1])\n",
    "        # One-hot encoding of actions taken to reach next state\n",
    "        actions = tf.placeholder(tf.float32, [None, self.action_space])\n",
    "        # The batch size changes as the experience buffer grows\n",
    "        batch_size = tf.placeholder(tf.int32)\n",
    "\n",
    "        delta = Q_target - tf.reshape(tf.reduce_sum(np.multiply(actions, self.nn_main), axis=1),\n",
    "                                      [batch_size, 1])\n",
    "\n",
    "        loss = tf.reduce_mean(0.5 * tf.square(delta), axis=0)\n",
    "        trainer = tf.train.RMSPropOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "        print('Starting training...\\n')\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            episode = 1\n",
    "            total_steps = 1\n",
    "            tf.global_variables_initializer().run()\n",
    "\n",
    "            # Array for computing moving average of discounted reward\n",
    "            ep_discounted_reward_mavg_arr = deque(maxlen=TRAINING_MOVING_AVG_ARR_LEN)\n",
    "            while total_steps <= self.num_iter:\n",
    "\n",
    "                step_num = 0\n",
    "                ep_discounted_reward = 0\n",
    "\n",
    "                # Create initial state by stacking the first frame\n",
    "                current_observation = self.env.reset()\n",
    "                current_frame = self.pre_process(current_observation)\n",
    "                self.state = np.stack((current_frame, current_frame, current_frame,\n",
    "                                       current_frame), axis=2)\n",
    "\n",
    "                while True:\n",
    "\n",
    "                    self.state = np.reshape(self.state, [1, 28, 28, 4])\n",
    "\n",
    "                    Q_val = sess.run(self.nn_main, feed_dict={self.X_main: self.state})\n",
    "                    action = np.argmax(Q_val)\n",
    "\n",
    "                    # Explore with probability EPSILON\n",
    "                    if np.random.uniform() < EPSILON:\n",
    "                        action = self.env.action_space.sample()\n",
    "\n",
    "                    next_observation, reward, is_done, info = self.env.step(action)\n",
    "                    next_frame = self.pre_process(next_observation)\n",
    "                    self.next_state = np.append(self.state[0, :, :, 1:],\n",
    "                                                np.reshape(next_frame, [28, 28, 1]), axis=2)\n",
    "\n",
    "                    # Clip rewards\n",
    "                    if reward <= -0.5:\n",
    "                        reward = -1\n",
    "                    elif -0.5 < reward <= 0.5:\n",
    "                        reward = 0\n",
    "                    elif reward > 0.5:\n",
    "                        reward = 1\n",
    "\n",
    "                    ep_discounted_reward += (self.gamma ** step_num) * reward\n",
    "                    ep_discounted_reward_mavg_arr.append(ep_discounted_reward)\n",
    "\n",
    "                    # Save experience to experience-replay buffer\n",
    "                    experience = [self.state[0], action, reward, is_done, self.next_state]\n",
    "                    self.experience_buff.append(experience)\n",
    "\n",
    "                    # Current batch size depends on whether the experience buffer is full or not\n",
    "                    if len(self.experience_buff) < self.batch_size:\n",
    "                        c_batch_size = len(self.experience_buff)\n",
    "                    else:\n",
    "                        c_batch_size = self.batch_size\n",
    "\n",
    "                    # ==============================================================================\n",
    "                    # Train from experience buffer\n",
    "                    # ==============================================================================\n",
    "\n",
    "                    if len(self.experience_buff) < self.experience_buff_size:\n",
    "                        batch = [self.experience_buff[i] for i in np.random.choice(\n",
    "                            len(self.experience_buff),\n",
    "                            c_batch_size,\n",
    "                            replace=False)]\n",
    "                    else:\n",
    "                        batch = [self.experience_buff[i] for i in np.random.choice(\n",
    "                            len(self.experience_buff),\n",
    "                            c_batch_size,\n",
    "                            replace=False)]\n",
    "\n",
    "                    batch_s_t = [m[0] for m in batch]  # State at time-step t\n",
    "                    batch_action = [m[1] for m in batch]  # State at time-step t\n",
    "                    batch_reward = [m[2] for m in batch]  # Reward for taking action a at time t\n",
    "                    batch_is_done = [m[3] for m in batch]  # Check if episode endoed\n",
    "                    batch_s_tn = [m[4] for m in batch]  # State at time-step t+1\n",
    "\n",
    "                    batch_reward = np.reshape(np.array(batch_reward), [c_batch_size, 1])\n",
    "\n",
    "                    # Q-value for next state\n",
    "                    Q_nVal = sess.run(self.nn_target, feed_dict={self.X_target: batch_s_tn})\n",
    "\n",
    "                    # Choose max of Q-value at state t+1\n",
    "                    Q_nVal_max = np.amax(Q_nVal, axis=1)\n",
    "\n",
    "                    # One-hot encoding of actions chosen\n",
    "                    batch_actions = np.zeros([c_batch_size, self.action_space])\n",
    "                    batch_actions[np.arange(c_batch_size), batch_action] = 1\n",
    "\n",
    "                    # If the episode has ended, then the total future reward should be zero i.e.\n",
    "                    # Q(s_t+1) = 0\n",
    "                    is_terminal_state = np.ones([c_batch_size, 1])\n",
    "                    is_terminal_state[np.where(batch_is_done)] = 0\n",
    "\n",
    "                    # Q-target for training in batch form\n",
    "                    batch_target_val = batch_reward + np.multiply(is_terminal_state,\n",
    "                                                                  self.gamma * np.transpose(\n",
    "                                                                      np.array([Q_nVal_max])))\n",
    "\n",
    "                    _, c_loss = sess.run([trainer, loss], feed_dict={Q_target: batch_target_val,\n",
    "                                                                     self.X_main: batch_s_t,\n",
    "                                                                     actions: batch_actions,\n",
    "                                                                     batch_size: c_batch_size})\n",
    "\n",
    "                    self.state = np.copy(self.next_state)\n",
    "\n",
    "                    # Save loss\n",
    "                    if total_steps % 100 == 0:\n",
    "                        data = [episode, total_steps, c_loss[0],\n",
    "                                np.mean(ep_discounted_reward_mavg_arr)]\n",
    "                        log(data, 'loss')\n",
    "                        print(data)\n",
    "\n",
    "                    # Evaluate agent and save performance\n",
    "                    if total_steps % 50000 == 0:\n",
    "                        data = [episode, total_steps, c_loss[0]] + ['%.4f' % elem for elem in\n",
    "                                                                    list(self.test(sess))]\n",
    "                        log(data, 'eval')\n",
    "\n",
    "                        print('Evaluation:\\n')\n",
    "                        print(data)\n",
    "\n",
    "                        save_model(sess)\n",
    "\n",
    "\n",
    "                    if step_num % 5000 == 0:\n",
    "                        sess.run(self.update_target_net)\n",
    "\n",
    "                    step_num += 1\n",
    "                    total_steps += 1\n",
    "\n",
    "                    if is_done:\n",
    "                        break\n",
    "                episode += 1\n",
    "        print('Finished training!!\\n')\n",
    "\n",
    "\n",
    "    def test(self, sess, render=False):\n",
    "        \"\"\"\n",
    "        Evaluate agent's performance.\n",
    "        :param sess:\n",
    "        :param render:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # List of episode lengths for each episode (number of time-steps)\n",
    "        ep_length = []\n",
    "        # Cumulative reward array to average over multiple runs\n",
    "        c_reward = []\n",
    "\n",
    "        for run in range(100):\n",
    "\n",
    "            step_num = 0\n",
    "            ep_c_reward = 0\n",
    "\n",
    "            # Create initial state by stacking the first frame\n",
    "            current_observation = self.env.reset()\n",
    "            current_frame = self.pre_process(current_observation)\n",
    "            state = np.stack((current_frame, current_frame, current_frame, current_frame), axis=2)\n",
    "\n",
    "            while True:\n",
    "\n",
    "                state = np.reshape(state, [1, 28, 28, 4])\n",
    "\n",
    "                if render:\n",
    "                    self.env.render()\n",
    "\n",
    "                actions = sess.run(self.nn_main, feed_dict={self.X_main: state})\n",
    "                action = np.argmax(actions)\n",
    "                next_observation, reward, is_done, _ = self.env.step(action)\n",
    "\n",
    "                # Clip rewards\n",
    "                if reward <= -0.5:\n",
    "                    reward = -1\n",
    "                elif -0.5 < reward <= 0.5:\n",
    "                    reward = 0\n",
    "                elif reward > 0.5:\n",
    "                    reward = 1\n",
    "\n",
    "                next_frame = self.pre_process(next_observation)\n",
    "                next_state = np.append(state[0, :, :, 1:], np.reshape(next_frame, [28, 28, 1]),\n",
    "                                       axis=2)\n",
    "                ep_c_reward += (self.gamma ** step_num) * reward\n",
    "\n",
    "                state = np.copy(next_state)\n",
    "                step_num += 1\n",
    "\n",
    "                if is_done:\n",
    "                    ep_length.append(step_num + 1)\n",
    "                    c_reward.append(ep_c_reward)\n",
    "                    break\n",
    "\n",
    "        return np.mean(c_reward), np.std(c_reward), np.mean(ep_length), np.std(ep_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(row, file):\n",
    "    \"\"\"\n",
    "    Writes training logs to CSV file.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if file == 'loss':\n",
    "        csv_writer = csv.writer(csv_loss_file)\n",
    "        csv_writer.writerow(row)\n",
    "\n",
    "    else:\n",
    "        csv_writer = csv.writer(csv_eval_file)\n",
    "        csv_writer.writerow(row)\n",
    "\n",
    "\n",
    "def save_model(sess):\n",
    "    \"\"\"\n",
    "    :param sess:\n",
    "    :param filename:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not os.path.exists(MODEL_FOLDER):\n",
    "        print('Creating path where to save model: ' + MODEL_FOLDER)\n",
    "\n",
    "        os.mkdir(MODEL_FOLDER)\n",
    "\n",
    "    print('Saving model at: ' + model_filename)\n",
    "    saver = tf.train.Saver(write_version=tf.train.SaverDef.V1)\n",
    "    saver.save(sess, MODEL_FOLDER + model_filename)\n",
    "    print('Model successfully saved.\\n')\n",
    "\n",
    "\n",
    "def load_model(sess, filename):\n",
    "    \"\"\"\n",
    "    :param sess:\n",
    "    :param filename:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        print('\\nLoading save model from: ' + filename)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, filename)\n",
    "        print('Model successfully loaded.\\n')\n",
    "        return True\n",
    "    else:\n",
    "        print('Model file <<' + filename + '>> does not exists!')\n",
    "        return False\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    game = AtariSan()\n",
    "\n",
    "    if isTRAIN:\n",
    "        global model_filename\n",
    "        global csv_eval_file, csv_loss_file\n",
    "\n",
    "        t = time.time()\n",
    "        ts = datetime.datetime.fromtimestamp(t).strftime('%Y%m%d - %H%M-%S')\n",
    "        csv_loss_filename = LOG_FOLDER + games[GAME_NUM][:-3] + '_' + 'loss' + '_' + ts + '.csv'\n",
    "        csv_eval_filename = LOG_FOLDER + games[GAME_NUM][:-3] + '_' + 'eval' + '_' + ts + '.csv'\n",
    "\n",
    "        model_filename = games[GAME_NUM][:-3] + '_' + ts\n",
    "\n",
    "        csv_loss_header = ['episode', 'total_steps', 'loss']\n",
    "        csv_eval_header = ['episode', 'total_steps', 'loss', 'reward_mean', 'reward_stddev',\n",
    "                           'episode_length_mean', 'episode_length_stddev']\n",
    "\n",
    "        with open(csv_loss_filename, 'w') as csv_loss_file, \\\n",
    "                open(csv_eval_filename, 'w') as csv_eval_file:\n",
    "            # Write meta-data and headers to CSV file\n",
    "            csv_writer = csv.writer(csv_loss_file)\n",
    "            csv_writer.writerow(csv_loss_header)\n",
    "\n",
    "            csv_writer = csv.writer(csv_eval_file)\n",
    "            csv_writer.writerow(csv_eval_header)\n",
    "\n",
    "            game.train()\n",
    "\n",
    "    else:\n",
    "        filename = MODEL_FOLDER + LOAD_MODEL_FILENAME\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            load_model(sess, filename)\n",
    "            game.test(sess, render=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
