{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook contains an implementation of Q-learning with an experience replay buffer.\n",
    "\n",
    "Author: Noorvir Aulakh\n",
    "Date: 04/03/2017\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import csv\n",
    "import gym\n",
    "import time\n",
    "import datetime\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from notbook_loader import *\n",
    "\n",
    "import agent_test\n",
    "import saver\n",
    "from random_q_learning import build_graph\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env._max_episode_steps = 301\n",
    "\n",
    "# ==================================================================================================\n",
    "# Parameters\n",
    "EPSILON = 0.05              # Exploration probability\n",
    "GAMMA = 0.99                # Discount factor\n",
    "MAX_ITER = 2000             # Number of epochs to run for\n",
    "LEARNING_RATE = 10 ** -4    # Step-size for optimiser\n",
    "EX_BUFFER_SIZE = 150000     # Size of experience replay buffer\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "NUM_TEST_RUNS = 10          # Number of runs to evaluate performance on\n",
    "NUM_EXPERIMENTS = 5         # Number of experiments to average over\n",
    "# ==================================================================================================\n",
    "\n",
    "# ==================================================================================================\n",
    "# Save Options\n",
    "LOG_FOLDER = './logs/'\n",
    "MODEL_FOLDER = './models/'\n",
    "\n",
    "LOAD_MODEL_FILENAME = 'experience_replay_q_learning.ckpt'\n",
    "# =================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ex_replay_q_learning(env):\n",
    "\n",
    "    # Get computation graph from random_q_learning.build_graph\n",
    "    X, DQN = build_graph(2, bias=True, dropout=False)\n",
    "\n",
    "    Q_target = tf.placeholder(tf.float32, [None, 1])\n",
    "    actions = tf.placeholder(tf.float32, [None, 2])  # One-hot encoding of actions taken\n",
    "    batch_size = tf.placeholder(tf.int32)  # The batch size changes and so doe\n",
    "\n",
    "    delta = Q_target - tf.reshape(tf.reduce_sum(np.multiply(actions, DQN), axis=1),\n",
    "                                  [batch_size, 1])\n",
    "    loss = tf.reduce_mean(0.5 * tf.square(delta), axis=0)\n",
    "    trainer = tf.train.RMSPropOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "    ex_replay_buf = collections.deque(maxlen=EX_BUFFER_SIZE)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        total_steps = 0\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        for episode in range(MAX_ITER):\n",
    "\n",
    "            state = env.reset()\n",
    "            reward = 0\n",
    "\n",
    "            for t in range(300):\n",
    "\n",
    "                Q_val = sess.run(DQN, feed_dict={X: [state]})\n",
    "                action = np.argmax(Q_val)\n",
    "\n",
    "                # Explore with probability EPSILON\n",
    "                if np.random.uniform() < EPSILON:\n",
    "                    action = env.action_space.sample()\n",
    "\n",
    "                n_state, _, is_done, _ = env.step(action)\n",
    "\n",
    "                if is_done:\n",
    "                    reward = -1\n",
    "\n",
    "                # Save experience to experience-replay buffer\n",
    "                experience = [state, action, reward, n_state]\n",
    "                ex_replay_buf.append(experience)\n",
    "\n",
    "                # Current batch size depends on whether the experience buffer is full or not\n",
    "                if len(ex_replay_buf) < BATCH_SIZE:\n",
    "                    C_BATCH_SIZE = len(ex_replay_buf)\n",
    "                else:\n",
    "                    C_BATCH_SIZE = BATCH_SIZE\n",
    "\n",
    "                # =================================================================================\n",
    "                # Train from experience buffer\n",
    "                # =================================================================================\n",
    "                if len(ex_replay_buf) < EX_BUFFER_SIZE:\n",
    "                    batch = [ex_replay_buf[i] for i in np.random.choice(len(ex_replay_buf),\n",
    "                                                                        C_BATCH_SIZE,\n",
    "                                                                        replace=False)]\n",
    "                else:\n",
    "                    batch = [ex_replay_buf[i] for i in np.random.choice(EX_BUFFER_SIZE,\n",
    "                                                                        C_BATCH_SIZE,\n",
    "                                                                        replace=False)]\n",
    "\n",
    "                batch_s_t = [m[0] for m in batch]  # State at time-step t\n",
    "                batch_action = [m[1] for m in batch]  # State at time-step t\n",
    "                batch_reward = [m[2] for m in batch]  # Reward for taking action a at time t\n",
    "                batch_s_tn = [m[3] for m in batch]  # State at time-step t+1\n",
    "\n",
    "                batch_reward = np.reshape(np.array(batch_reward), [C_BATCH_SIZE, 1])\n",
    "\n",
    "                # Q-value for next state\n",
    "                Q_nVal = sess.run(DQN, feed_dict={X: batch_s_tn})\n",
    "\n",
    "                # Choose max of Q-value at state t+1\n",
    "                Q_nVal_max = np.amax(Q_nVal, axis=1)\n",
    "\n",
    "                # One-hot encoding of actions chosen\n",
    "                batch_actions = np.zeros([C_BATCH_SIZE, 2])\n",
    "                batch_actions[np.arange(C_BATCH_SIZE), batch_action] = 1\n",
    "\n",
    "                # If the episode has ended, then the total future reward should be zero Q(s_t+1) = 0\n",
    "                is_terminal_state = np.ones([C_BATCH_SIZE, 1])\n",
    "                is_terminal_state[np.where(batch_reward == -1)] = 0\n",
    "\n",
    "                batch_target_val = batch_reward + np.multiply(is_terminal_state,\n",
    "                                                              GAMMA * np.transpose(\n",
    "                                                                        np.array([Q_nVal_max])))\n",
    "\n",
    "                _, c_loss = sess.run([trainer, loss], feed_dict={Q_target: batch_target_val,\n",
    "                                                                X: batch_s_t,\n",
    "                                                                 actions: batch_actions,\n",
    "                                                                 batch_size: C_BATCH_SIZE})\n",
    "\n",
    "                total_steps += 1\n",
    "                state = np.copy(n_state)\n",
    "\n",
    "                if is_done:\n",
    "                    break\n",
    "\n",
    "            if (episode + 1) % 20 == 0:\n",
    "                data = [episode + 1, total_steps, c_loss[0]]\n",
    "                # Log the loss and reward to CSV file\n",
    "                print(data)\n",
    "\n",
    "\n",
    "            # Evaluate performance and log to CSV file\n",
    "            if (episode + 1) % 20 == 0:\n",
    "                data = [episode + 1, total_steps, c_loss[0]] + ['%.4f' % elem for elem in\n",
    "                                                                list(agent_test.test(X, DQN, sess,\n",
    "                                                                                  NUM_TEST_RUNS))]\n",
    "                print('Evaluation:')\n",
    "                print(data)\n",
    "\n",
    "            if (episode + 1) % 500 == 0:\n",
    "                saver.save_model(sess, model_filename)\n",
    "\n",
    "        saver.save_model(sess, model_filename)\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env):\n",
    "    # ==============================================================================================\n",
    "    # Initialise Log writer\n",
    "    # ==============================================================================================\n",
    "\n",
    "    global model_filename\n",
    "    global csv_loss_file, csv_eval_file\n",
    "\n",
    "    t = time.time()\n",
    "    ts = datetime.datetime.fromtimestamp(t).strftime('%Y-%m-%d-%H%M-%S')\n",
    "    csv_loss_filename = LOG_FOLDER + 'experience_replay_q_learning' + '_' + 'loss' + '_' + ts + '.csv'\n",
    "    csv_eval_filename = LOG_FOLDER + 'experience_replay_q_learning' + '_' + 'eval' + '_' + ts + '.csv'\n",
    "\n",
    "    model_filename = 'experience_replay_q_learning' + '_' + ts\n",
    "\n",
    "    csv_loss_header = ['episode', 'total_steps', 'loss']\n",
    "    csv_eval_header = ['episode', 'total_steps', 'loss', 'reward_mean', 'reward_stddev',\n",
    "                       'episode_length_mean', 'episode_length_stddev']\n",
    "\n",
    "    with open(csv_loss_filename, 'w') as csv_loss_file, \\\n",
    "            open(csv_eval_filename, 'w') as csv_eval_file:\n",
    "        # Write meta-data and headers to CSV file\n",
    "        csv_writer = csv.writer(csv_loss_file)\n",
    "        csv_writer.writerow(csv_loss_header)\n",
    "\n",
    "        csv_writer = csv.writer(csv_eval_file)\n",
    "        csv_writer.writerow(csv_eval_header)\n",
    "\n",
    "        for experiment in range(4):\n",
    "            print('Experiment number: ', experiment)\n",
    "            ex_replay_q_learning(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    filename = MODEL_FOLDER + LOAD_MODEL_FILENAME\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        X, DQN = build_graph(2, bias=True, dropout=False)\n",
    "        tf.global_variables_initializer().run()\n",
    "        helpers.load_model(sess, filename)\n",
    "        helpers.test(env, X, DQN, sess, 10, render=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
