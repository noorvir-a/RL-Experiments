{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook contains an implementation of the double Q-learning algorithm.\n",
    "\n",
    "N.B. This is an untested and incomplete implementation.\n",
    "\n",
    "Author: Noorvir Aulakh\n",
    "Date: 10/03/2017\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import csv\n",
    "import gym\n",
    "import time\n",
    "import datetime\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import saver\n",
    "import agent_test\n",
    "from notbook_loader import *\n",
    "\n",
    "from target_network_q_learning import build_graph\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env._max_episode_steps = 301\n",
    "    \n",
    "# ==================================================================================================\n",
    "# Parameters\n",
    "\n",
    "isTRAIN = True\n",
    "\n",
    "EPSILON = 0.05              # Exploration probability\n",
    "GAMMA = 0.99                # Discount factor\n",
    "MAX_ITER = 2000             # Number of epochs to run for\n",
    "LEARNING_RATE = 10 ** -4    # Step-size for optimiser\n",
    "EX_BUFFER_SIZE = 150000     # Size of experience replay buffer\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "NUM_TEST_RUNS = 10          # Number of runs to evaluate performance on\n",
    "NUM_EXPERIMENTS = 5         # Number of experiments to average over\n",
    "# ==================================================================================================\n",
    "\n",
    "# ==================================================================================================\n",
    "# Save Options\n",
    "LOG_FOLDER = './logs/'\n",
    "MODEL_FOLDER = './models/'\n",
    "\n",
    "LOAD_MODEL_FILENAME = 'double_q_learning.ckpt'\n",
    "# =================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_q_learning(env):\n",
    "\n",
    "    with tf.variable_scope('DQN_main'):\n",
    "        X_main, weights_main, DQN_main = build_graph()\n",
    "        W1, b1, W2, b2 = weights_main\n",
    "\n",
    "    with tf.variable_scope('DQN_target'):\n",
    "        X_target, weights_target, DQN_target = build_graph()\n",
    "        W1_t, b1_t, W2_t, b2_t = weights_target\n",
    "\n",
    "    updateOp = [W1_t.assign(W1), b1_t.assign(b1), W2_t.assign(W2), b2_t.assign(b2)]\n",
    "\n",
    "    Q_target = tf.placeholder(tf.float32, [1, 2])\n",
    "\n",
    "    loss = 0.5 * tf.reduce_sum(tf.square(Q_target - DQN_main), axis=1)\n",
    "    trainer = tf.train.RMSPropOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        total_steps = 0\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        for episode in range(MAX_ITER):\n",
    "\n",
    "            state = env.reset()\n",
    "            reward = 0\n",
    "            is_terminal_state = 0\n",
    "\n",
    "            for t in range(300):\n",
    "\n",
    "                Q_val = sess.run(DQN_main, feed_dict={X_main: [state]})\n",
    "                target = np.copy(Q_val[0])\n",
    "                action = np.argmax(Q_val)\n",
    "\n",
    "                # Explore with probability EPSILON\n",
    "                if np.random.uniform() < EPSILON:\n",
    "                    action = env.action_space.sample()\n",
    "\n",
    "                n_state, _, is_done, _ = env.step(action)\n",
    "\n",
    "                if is_done:\n",
    "                    reward = -1\n",
    "                    is_terminal_state = 1\n",
    "\n",
    "                # Choose action at next state with the primary network\n",
    "                Q_nVal = sess.run(DQN_main, feed_dict={X_main: [n_state]})\n",
    "                n_action = np.argmax(Q_nVal)\n",
    "\n",
    "                # Use target network to calculate Q-value for this action\n",
    "                Q_nVal_t = sess.run(DQN_target, feed_dict={X_target: [n_state]})\n",
    "\n",
    "                target[action] = reward + (1 - is_terminal_state) * GAMMA * Q_nVal_t[0][n_action]\n",
    "                _, c_loss = sess.run([trainer, loss], feed_dict={X_main: [state],\n",
    "                                                                 Q_target: [target]})\n",
    "\n",
    "                state = np.copy(n_state)\n",
    "                total_steps += 1\n",
    "\n",
    "\n",
    "                if is_done:\n",
    "                    break\n",
    "\n",
    "            if (episode + 1) % 5 == 0:\n",
    "                sess.run(updateOp)\n",
    "\n",
    "            # Log the loss and reward to CSV file\n",
    "            data = [episode + 1, total_steps, c_loss[0]]\n",
    "\n",
    "            if (episode + 1) % 20 == 0:\n",
    "                print(data)\n",
    "\n",
    "            # Evaluate performance and log to CSV file\n",
    "            if (episode + 1) % 20 == 0:\n",
    "                data = [episode + 1, total_steps, c_loss[0]] + ['%.4f' % elem for elem in\n",
    "                                                                list(agent_test.test(X_main,\n",
    "                                                                                  DQN_main,\n",
    "                                                                                  sess,\n",
    "                                                                                  NUM_TEST_RUNS))]\n",
    "                print('Evaluation:')\n",
    "                print(data)\n",
    "\n",
    "            if (episode + 1) % 500 == 0:\n",
    "                saver.save_model(sess, model_filename)\n",
    "\n",
    "        saver.save_model(sess, model_filename)\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env):\n",
    "\n",
    "    # ==============================================================================================\n",
    "    # Initialise Log writer\n",
    "    # ==============================================================================================\n",
    "    global model_filename\n",
    "    global csv_loss_file, csv_eval_file\n",
    "\n",
    "    t = time.time()\n",
    "    ts = datetime.datetime.fromtimestamp(t).strftime('%Y-%m-%d--%H%M-%S')\n",
    "    csv_loss_filename = LOG_FOLDER + 'double_q_learning' + '_' + 'loss' + '_' + ts + '.csv'\n",
    "    csv_eval_filename = LOG_FOLDER + 'double_q_learning' + '_' + 'eval' + '_' + ts + '.csv'\n",
    "\n",
    "    model_filename = 'double_q_learning' + '_' + ts\n",
    "\n",
    "    csv_loss_header = ['episode', 'total_steps', 'loss']\n",
    "    csv_eval_header = ['episode', 'total_steps', 'loss', 'reward_mean', 'reward_stddev',\n",
    "                       'episode_length_mean', 'episode_length_stddev']\n",
    "\n",
    "    with open(csv_loss_filename, 'w') as csv_loss_file, \\\n",
    "            open(csv_eval_filename, 'w') as csv_eval_file:\n",
    "        # Write meta-data and headers to CSV file\n",
    "        csv_writer = csv.writer(csv_loss_file)\n",
    "        csv_writer.writerow(csv_loss_header)\n",
    "\n",
    "        csv_writer = csv.writer(csv_eval_file)\n",
    "        csv_writer.writerow(csv_eval_header)\n",
    "\n",
    "        for experiment in range(NUM_EXPERIMENTS):\n",
    "            print('Experiment number: ', experiment)\n",
    "            double_q_learning(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Agent\n",
    "train(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Agent\n",
    "filename = MODEL_FOLDER + LOAD_MODEL_FILENAME\n",
    "\n",
    "with tf.Session() as sess:\n",
    "        X, _, DQN = build_graph()\n",
    "        tf.global_variables_initializer().run()\n",
    "        saver.load_model(sess, filename)\n",
    "        agent_test.test(X, DQN, sess, 10, render=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
