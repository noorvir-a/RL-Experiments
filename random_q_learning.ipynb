{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This files experiments with a Q-learning policy trained on randomly sampled roll-outs. \n",
    "\n",
    "Author: Noorvir Aulakh\n",
    "Date: 01/03/2017\n",
    "\"\"\"\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "import csv\n",
    "\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import agent_test\n",
    "import saver\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env._max_episode_steps = 301\n",
    "\n",
    "    \n",
    "learning_rates = [10**-5, 10**-4, 10**-3, 10**-2, 10**-1, 0.5]\n",
    "# ==================================================================================================\n",
    "# Parameters\n",
    "\n",
    "# Part 1 - Linear model with one output per action\n",
    "# Part 2 - Hidden layer (linear + ReLU) followed by linear layer with one output per action\n",
    "PART_NUM = 1\n",
    "\n",
    "LEARNING_RATE_INDEX = 5         # index of learning_rates array\n",
    "\n",
    "GAMMA = 0.99                    # Discount factor\n",
    "MAX_ITER = 2000                 # Number of epochs to run for\n",
    "\n",
    "# Step-size for optimiser\n",
    "LEARNING_RATE = learning_rates[LEARNING_RATE_INDEX]\n",
    "NUM_TRAINING_SAMPLES = 2000\n",
    "BATCH_SIZE = 25\n",
    "\n",
    "NUM_TEST_RUNS = 10              # Number of runs to evaluate performance on\n",
    "NUM_EXPERIMENTS = 10            # Number of experiments to average over\n",
    "# ==================================================================================================\n",
    "\n",
    "# ==================================================================================================\n",
    "# Save Options\n",
    "LOG_FOLDER = './logs/'\n",
    "MODEL_FOLDER = './models/'\n",
    "LOAD_MODEL_FILENAME = 'random_q_learning_part1.ckpt'\n",
    "# =================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_episodes(env):\n",
    "    \"\"\"\n",
    "    Collect 2000 episodes under a uniform-random policy\n",
    "    :return: episodes - dictionary containing history of agent over 200 episodes\n",
    "    \"\"\"\n",
    "\n",
    "    episodes = []  # dictionary to store episode history\n",
    "\n",
    "    for episode_num in range(NUM_TRAINING_SAMPLES):\n",
    "\n",
    "        state = env.reset()\n",
    "        for t in range(300):\n",
    "\n",
    "            action = env.action_space.sample()\n",
    "            res_state, _, is_done, _ = env.step(action)\n",
    "\n",
    "            reward = 0\n",
    "            episode = [state, action, reward, res_state]\n",
    "\n",
    "            if is_done:\n",
    "                reward = -1\n",
    "                episode = [state, action, reward, res_state]\n",
    "                episodes.append(episode)\n",
    "                break\n",
    "\n",
    "            episodes.append(episode)\n",
    "            state = res_state\n",
    "\n",
    "    return episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(graph_type, init_type='xavier', num_hidden=100, bias=False, dropout=True,\n",
    "                                                                                keep_prob=0.7):\n",
    "    \"\"\"\n",
    "    Create tensorflow model: 1) one linear layer with one output per action, 2) a hidden layer(100)\n",
    "    - linear transformation + ReLU - followed by a linear layer with one output per action.\n",
    "    :param graph_type: 1 for linear, 2 hidden layer graph\n",
    "    :param init_type:\n",
    "    :param num_hidden:\n",
    "    :param bias:\n",
    "    :param dropout:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    if graph_type == 1:\n",
    "\n",
    "        W = tf.Variable(tf.truncated_normal([4, 2]), name='W')\n",
    "        X = tf.placeholder(tf.float32, [None, 4], name='X')\n",
    "\n",
    "        return X, tf.matmul(X, W)\n",
    "\n",
    "    elif graph_type == 2:\n",
    "\n",
    "        X = tf.placeholder(tf.float32, [None, 4], name='X')\n",
    "\n",
    "        if init_type == 'truncated_normal':\n",
    "            initializer = tf.truncated_normal_initializer(stddev=0.01)\n",
    "\n",
    "        elif init_type == 'random_normal':\n",
    "            initializer = tf.random_normal_initializer(stddev=0.01)\n",
    "\n",
    "        elif init_type == 'xavier':\n",
    "            initializer = tf.contrib.layers.xavier_initializer()\n",
    "\n",
    "        W1 = tf.get_variable('W1', [4, num_hidden], initializer=initializer)\n",
    "        b1 = tf.get_variable('b1', [num_hidden], initializer=initializer)\n",
    "\n",
    "        W2 = tf.get_variable('W2', [num_hidden, 2], initializer=initializer)\n",
    "        b2 = tf.get_variable('b2', [2], initializer=initializer)\n",
    "\n",
    "        if bias:\n",
    "            l1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "            relu_l = l1\n",
    "            if dropout:\n",
    "                relu_l = tf.nn.dropout(l1, keep_prob=keep_prob)\n",
    "            output_l = tf.matmul(relu_l, W2) + b2\n",
    "        else:\n",
    "            l1 = tf.nn.relu(tf.matmul(X, W1))\n",
    "            relu_l = l1\n",
    "            if dropout:\n",
    "                relu_l = tf.nn.dropout(l1, keep_prob=keep_prob)\n",
    "            output_l = tf.matmul(relu_l, W2)\n",
    "\n",
    "        return X, output_l\n",
    "\n",
    "    else:\n",
    "        print('Unknown graph-type specified')\n",
    "        return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_q_learning(env):\n",
    "    \"\"\"\n",
    "    Implement batch Q-learning using random experience alone over 2000 episodes.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    episodes = collect_episodes(env)\n",
    "\n",
    "    Q_target = tf.placeholder(tf.float32, [None, 1])\n",
    "    actions = tf.placeholder(tf.float32, [None, 2])\n",
    "\n",
    "    # Get computation graph\n",
    "    X, DQN = build_graph(PART_NUM)\n",
    "\n",
    "    delta = Q_target - tf.reshape(tf.reduce_sum(np.multiply(actions, DQN), axis=1),\n",
    "                                  [BATCH_SIZE, 1])\n",
    "    loss = tf.reduce_mean(0.5 * tf.square(delta), axis=0)\n",
    "\n",
    "    trainer = tf.train.RMSPropOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "    print('Starting training... \\n')\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        total_steps = 0\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        for episode in range(MAX_ITER):\n",
    "\n",
    "            # Initialise mini-batch start and end indices\n",
    "            mb_st = 0\n",
    "            np.random.shuffle(episodes)\n",
    "\n",
    "            for batch_num in range(int(NUM_TRAINING_SAMPLES / BATCH_SIZE)):\n",
    "\n",
    "                # Select batch\n",
    "                batch = [episodes[mb_st + i] for i in range(BATCH_SIZE)]\n",
    "                batch_s_t = [m[0] for m in batch]       # State at time-step t\n",
    "                batch_action = [m[1] for m in batch]    # State at time-step t\n",
    "                batch_reward = [m[2] for m in batch]    # Reward for taking action a at time t\n",
    "                batch_s_tn = [m[3] for m in batch]      # State at time-step t+1\n",
    "\n",
    "                batch_reward = np.reshape(np.array(batch_reward), [BATCH_SIZE, 1])\n",
    "\n",
    "                # Q-value for next state\n",
    "                Q_nVal = sess.run(DQN, feed_dict={X: batch_s_tn})\n",
    "\n",
    "                # Choose max of Q-value at state t+1\n",
    "                Q_nVal_max = np.amax(Q_nVal, axis=1)\n",
    "\n",
    "                # One-hot encoding of actions chosen\n",
    "                batch_actions = np.zeros([BATCH_SIZE, 2])\n",
    "                batch_actions[np.arange(BATCH_SIZE), batch_action] = 1\n",
    "\n",
    "                # If the episode has ended, then the total future reward should be zero Q(s_t+1) = 0\n",
    "                is_terminal_state = np.ones([BATCH_SIZE, 1])\n",
    "                is_terminal_state[np.where(batch_reward == -1)] = 0\n",
    "\n",
    "                batch_target_val = batch_reward + np.multiply(is_terminal_state,\n",
    "                                                              GAMMA * np.transpose(\n",
    "                                                                  np.array([Q_nVal_max])))\n",
    "\n",
    "                _, c_loss = sess.run([trainer, loss], feed_dict={Q_target: batch_target_val,\n",
    "                                                                 X: batch_s_t,\n",
    "                                                                 actions: batch_actions})\n",
    "\n",
    "                mb_st += BATCH_SIZE\n",
    "                total_steps += 1\n",
    "\n",
    "            if (episode + 1) % 10 == 0:\n",
    "                data = [episode + 1, total_steps, c_loss[0]]\n",
    "                # Log the loss and reward to CSV file\n",
    "                print(data)\n",
    "\n",
    "            # Evaluate performance and log to CSV file\n",
    "            if (episode + 1) % 50 == 0:\n",
    "                data = [episode + 1, total_steps, c_loss[0]] + ['%.4f' % elem for elem in\n",
    "                                                                list(agent_test.test(X, DQN, sess,\n",
    "                                                                                  NUM_TEST_RUNS))]\n",
    "                print('Evaluation:')\n",
    "                print(data)\n",
    "\n",
    "            if (episode + 1) % 500 == 0:\n",
    "                saver.save_model(sess, model_filename)\n",
    "\n",
    "        saver.save_model(sess, model_filename)\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env):\n",
    "    # ==============================================================================================\n",
    "    # Initialise Log writer\n",
    "    # ==============================================================================================\n",
    "\n",
    "    global model_filename\n",
    "    global csv_loss_file, csv_eval_file\n",
    "\n",
    "    t = time.time()\n",
    "    ts = datetime.datetime.fromtimestamp(t).strftime('%Y-%m-%d--%H%M-%S')\n",
    "    csv_loss_filename = LOG_FOLDER + 'A3_lin_' + str(LEARNING_RATE_INDEX) + '_loss_' + ts + '.csv'\n",
    "    csv_eval_filename = LOG_FOLDER + 'A3_lin_' + str(LEARNING_RATE_INDEX) + '_eval_' + ts + '.csv'\n",
    "\n",
    "    model_filename = 'A3_lin_' + str(LEARNING_RATE_INDEX) + '_' + ts\n",
    "\n",
    "    csv_loss_header = ['episode', 'total_steps', 'loss']\n",
    "    csv_eval_header = ['episode', 'total_steps', 'loss', 'reward_mean', 'reward_stddev',\n",
    "                       'episode_length_mean', 'episode_length_stddev']\n",
    "\n",
    "    with open(csv_loss_filename, 'w') as csv_loss_file, \\\n",
    "            open(csv_eval_filename, 'w') as csv_eval_file:\n",
    "\n",
    "        # Write meta-data and headers to CSV file\n",
    "        csv_writer = csv.writer(csv_loss_file)\n",
    "        csv_writer.writerow(csv_loss_header)\n",
    "\n",
    "        csv_writer = csv.writer(csv_eval_file)\n",
    "        csv_writer.writerow(csv_eval_header)\n",
    "\n",
    "        for experiment in range(NUM_EXPERIMENTS):\n",
    "            print('Experiment number: ', experiment)\n",
    "            random_q_learning(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    filename = MODEL_FOLDER + LOAD_MODEL_FILENAME\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        if PART_NUM == 1:\n",
    "            X, DQN = build_graph(1)\n",
    "        else:\n",
    "            X, DQN = build_graph(2)\n",
    "\n",
    "        tf.global_variables_initializer().run()\n",
    "        saver.load_model(sess, filename)\n",
    "        saver.test(env, X, DQN, sess, 100, render=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
