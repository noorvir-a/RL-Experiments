{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This file contains an implementation of vanilla Q-learning.\n",
    "\n",
    "Author: Noorvir Aulakh\n",
    "Date: 04/03/2017\n",
    "\"\"\"\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "\n",
    "import saver\n",
    "import agent_test\n",
    "\n",
    "# ==================================================================================================\n",
    "# Parameters\n",
    "\n",
    "EPSILON = 0.05                  # Exploration probability\n",
    "GAMMA = 0.99                    # Discount factor\n",
    "MAX_ITER = 2000                 # Number of epochs to run for\n",
    "LEARNING_RATE = 10**-4          # Step-size for optimiser\n",
    "\n",
    "NUM_TEST_RUNS = 10              # Number of runs to evaluate performance on\n",
    "NUM_EXPERIMENTS = 100            # Number of experiments to average over\n",
    "# ==================================================================================================\n",
    "\n",
    "# ==================================================================================================\n",
    "# Save Options\n",
    "LOG_FOLDER = './logs/'\n",
    "MODEL_FOLDER = './models/'\n",
    "\n",
    "LOAD_MODEL_FILENAME = 'online_q_learning.ckpt'\n",
    "# =================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(num_hidden=100):\n",
    "    \"\"\"\n",
    "    Create tensorflow model: 1) one linear layer with one output per action, 2) a hidden layer(100)\n",
    "    - linear transformation + ReLU - followed by a linear layer with one output per action.\n",
    "    :param num_hidden:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    X = tf.placeholder(tf.float32, [1, 4], name='X')\n",
    "    W1 = tf.Variable(tf.random_normal([4, num_hidden], stddev=0.01))\n",
    "    b1 = tf.Variable(tf.random_normal([num_hidden], stddev=0.01))\n",
    "\n",
    "    W2 = tf.Variable(tf.random_normal([num_hidden, 2], stddev=0.01))\n",
    "    b2 = tf.Variable(tf.random_normal([2], stddev=0.01))\n",
    "\n",
    "    relu_l = tf.nn.dropout(tf.nn.relu(tf.matmul(X, W1)), keep_prob=0.8)\n",
    "\n",
    "    return X, tf.matmul(relu_l, W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_q_learning(env, num_hidden, csv_loss_file, csv_eval_file, model_filename):\n",
    "    \"\"\"\n",
    "    Implement an online Q learning algorithm with a small neural net for function approximation.\n",
    "    :param env:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    Q_target = tf.placeholder(tf.float32, [1, 2])\n",
    "\n",
    "    # Get computation graph\n",
    "    X, DQN = build_graph(num_hidden=num_hidden)\n",
    "\n",
    "    loss = 0.5 * tf.reduce_sum(tf.square(Q_target - DQN), axis=1)\n",
    "    trainer = tf.train.RMSPropOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        total_steps = 0\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        for episode in range(MAX_ITER):\n",
    "\n",
    "            state = env.reset()\n",
    "            reward = 0\n",
    "            is_terminal_state = 0\n",
    "\n",
    "            for step_num in range(300):\n",
    "\n",
    "                Q_val = sess.run(DQN, feed_dict={X: [state]})\n",
    "\n",
    "                target = np.copy(Q_val[0])\n",
    "                action = np.argmax(Q_val)\n",
    "\n",
    "                # Explore with probability EPSILON\n",
    "                if np.random.uniform() < EPSILON:\n",
    "                    action = env.action_space.sample()\n",
    "\n",
    "                n_state, _, is_done, _ = env.step(action)\n",
    "\n",
    "                if is_done:\n",
    "                    reward = -1\n",
    "                    is_terminal_state = 1\n",
    "\n",
    "                # Get Q-values over the next state\n",
    "                Q_nVal = sess.run(DQN, feed_dict={X: [n_state]})\n",
    "\n",
    "                # Calculate target Q-value\n",
    "                target[action] = reward + (1 - is_terminal_state) * GAMMA * np.max(Q_nVal)\n",
    "\n",
    "                _, c_loss = sess.run([trainer, loss], feed_dict={X: [state], Q_target: [target]})\n",
    "\n",
    "                state = np.copy(n_state)\n",
    "                total_steps += 1\n",
    "\n",
    "                if is_done:\n",
    "                    break\n",
    "\n",
    "            if (episode + 1) % 1 == 0:\n",
    "                data = [episode + 1, total_steps, c_loss[0]]\n",
    "                print(data)\n",
    "\n",
    "            # Evaluate performance and log to CSV file\n",
    "            if (episode + 1) % 1 == 0:\n",
    "                data = [episode + 1, total_steps, c_loss[0]] + ['%.4f' % elem for elem in\n",
    "                                                                list(agent_test.test(X, DQN, sess, \n",
    "                                                                          NUM_TEST_RUNS))]\n",
    "                print('Evaluation: \\n')\n",
    "                print(data)\n",
    "\n",
    "            if (episode + 1) % 500 == 0:\n",
    "                saver.save_model(sess, model_filename)\n",
    "\n",
    "        saver.save_model(sess, model_filename)\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env):\n",
    "    # ==============================================================================================\n",
    "    # Initialise Log writer\n",
    "    # ==============================================================================================\n",
    "\n",
    "    t = time.time()\n",
    "    ts = datetime.datetime.fromtimestamp(t).strftime('%Y-%m-%d--%H%M-%S')\n",
    "    model_filename = 'online_q_learning' + '_' + ts\n",
    "    \n",
    "    csv_loss_filename = LOG_FOLDER + 'online_q_learning' + '_' + 'loss' + '_' + ts + '.csv'\n",
    "    csv_eval_filename = LOG_FOLDER + 'online_q_learning' + '_' + 'eval' + '_' + ts + '.csv'\n",
    "    csv_loss_file = \"\"\n",
    "    csv_eval_file = \"\"\n",
    "    \n",
    "    csv_loss_header = ['episode', 'total_steps', 'loss']\n",
    "    csv_eval_header = ['episode', 'total_steps', 'loss', 'reward_mean', 'reward_stddev',\n",
    "                       'episode_length_mean', 'episode_length_stddev']\n",
    "\n",
    "    with open(csv_loss_filename, 'w') as csv_loss_file, \\\n",
    "            open(csv_eval_filename, 'w') as csv_eval_file:\n",
    "\n",
    "        # Write meta-data and headers to CSV file\n",
    "        csv_writer = csv.writer(csv_loss_file)\n",
    "        csv_writer.writerow(csv_loss_header)\n",
    "\n",
    "        csv_writer = csv.writer(csv_eval_file)\n",
    "        csv_writer.writerow(csv_eval_header)\n",
    "\n",
    "    for experiment in range(3):\n",
    "        print('Experiment number: ', experiment)\n",
    "        online_q_learning(env, 100, csv_loss_file, csv_eval_file, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('CartPole-v0')\n",
    "    env._max_episode_steps = 301        # need this hack to ensure the environment doesn't quit on the 300th step\n",
    "\n",
    "    train(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test network.\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    filename = MODEL_FOLDER + LOAD_MODEL_FILENAME\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        X, DQN = build_graph(num_hidden=100)\n",
    "        tf.global_variables_initializer().run()\n",
    "        saver.load_model(sess, filename)\n",
    "        saver.test(env, X, DQN, sess, 10, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
