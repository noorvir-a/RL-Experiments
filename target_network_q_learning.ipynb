{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook contains an implementation of Target Network Q Learning - a trick employed to make \n",
    "Q-learning work better. \n",
    "\n",
    "Author: Noorvir Aulakh\n",
    "Date: 07/03/2017\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import gym\n",
    "import time\n",
    "import datetime\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import saver\n",
    "import agent_test\n",
    "from notbook_loader import *\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env._max_episode_steps = 301\n",
    "\n",
    "# ==================================================================================================\n",
    "# Parameters\n",
    "\n",
    "isTRAIN = True\n",
    "\n",
    "EPSILON = 0.05              # Exploration probability\n",
    "GAMMA = 0.99                # Discount factor\n",
    "MAX_ITER = 2000             # Number of epochs to run for\n",
    "LEARNING_RATE = 10 ** -4    # Step-size for optimiser\n",
    "EX_BUFFER_SIZE = 150000     # Size of experience replay buffer\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "NUM_TEST_RUNS = 10          # Number of runs to evaluate performance on\n",
    "NUM_EXPERIMENTS = 5         # Number of experiments to average over\n",
    "# ==================================================================================================\n",
    "\n",
    "# ==================================================================================================\n",
    "# Save Options\n",
    "LOG_FOLDER = './logs/'\n",
    "MODEL_FOLDER = './models/'\n",
    "\n",
    "LOAD_MODEL_FILENAME = 'target_network_q_learning.ckpt'\n",
    "# ==================================================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph():\n",
    "    \"\"\"\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, [None, 4], name='X')\n",
    "\n",
    "    W1 = tf.get_variable('W1', [4, 100], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b1 = tf.get_variable('b1', [100], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    W2 = tf.get_variable('W2', [100, 2], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b2 = tf.get_variable('b2', [2], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "    relu_l = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    weights = [W1, b1, W2, b2]\n",
    "\n",
    "    return X, weights, tf.matmul(relu_l, W2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_net_q_learning(env):\n",
    "\n",
    "    with tf.variable_scope('DQN_main'):\n",
    "        X_main, weights_main, DQN_main = build_graph()\n",
    "        W1, b1, W2, b2 = weights_main\n",
    "\n",
    "    with tf.variable_scope('DQN_target'):\n",
    "        X_target, weights_target, DQN_target = build_graph()\n",
    "        W1_t, b1_t, W2_t, b2_t = weights_target\n",
    "\n",
    "    updateOp = [W1_t.assign(W1), b1_t.assign(b1), W2_t.assign(W2), b2_t.assign(b2)]\n",
    "\n",
    "    Q_target = tf.placeholder(tf.float32, [None, 1])\n",
    "    actions = tf.placeholder(tf.float32, [None, 2])         # One-hot encoding of actions taken\n",
    "    batch_size = tf.placeholder(tf.int32)                   # The batch size changes and so doe\n",
    "\n",
    "    delta = Q_target - tf.reshape(tf.reduce_sum(np.multiply(actions, DQN_main), axis=1),\n",
    "                                  [batch_size, 1])\n",
    "\n",
    "    loss = tf.reduce_mean(0.5 * tf.square(delta), axis=0)\n",
    "    trainer = tf.train.RMSPropOptimizer(LEARNING_RATE).minimize(loss)\n",
    "\n",
    "    ex_replay_buf = collections.deque(maxlen=EX_BUFFER_SIZE)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        total_steps = 0\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        for episode in range(MAX_ITER):\n",
    "\n",
    "            state = env.reset()\n",
    "            reward = 0\n",
    "\n",
    "            for t in range(300):\n",
    "\n",
    "                Q_val = sess.run(DQN_main, feed_dict={X_main: [state]})\n",
    "                action = np.argmax(Q_val)\n",
    "\n",
    "                # Explore with probability EPSILON\n",
    "                if np.random.uniform() < EPSILON:\n",
    "                    action = env.action_space.sample()\n",
    "\n",
    "                n_state, _, is_done, _ = env.step(action)\n",
    "\n",
    "                if is_done:\n",
    "                    reward = -1\n",
    "\n",
    "                # Save experience to experience-replay buffer\n",
    "                experience = [state, action, reward, n_state]\n",
    "                ex_replay_buf.append(experience)\n",
    "\n",
    "                # Current batch size depends on whether the experience buffer is full or not\n",
    "                if len(ex_replay_buf) < BATCH_SIZE:\n",
    "                    C_BATCH_SIZE = len(ex_replay_buf)\n",
    "                else:\n",
    "                    C_BATCH_SIZE = BATCH_SIZE\n",
    "\n",
    "                # =================================================================================\n",
    "                # Train from experience buffer\n",
    "                # =================================================================================\n",
    "                if len(ex_replay_buf) < EX_BUFFER_SIZE:\n",
    "                    batch = [ex_replay_buf[i] for i in np.random.choice(len(ex_replay_buf),\n",
    "                                                                        C_BATCH_SIZE,\n",
    "                                                                        replace=False)]\n",
    "                else:\n",
    "                    batch = [ex_replay_buf[i] for i in np.random.choice(EX_BUFFER_SIZE,\n",
    "                                                                        C_BATCH_SIZE,\n",
    "                                                                        replace=False)]\n",
    "\n",
    "                batch_s_t = [m[0] for m in batch]           # State at time-step t\n",
    "                batch_action = [m[1] for m in batch]        # State at time-step t\n",
    "                batch_reward = [m[2] for m in batch]        # Reward for taking action a at time t\n",
    "                batch_s_tn = [m[3] for m in batch]          # State at time-step t+1\n",
    "\n",
    "                batch_reward = np.reshape(np.array(batch_reward), [C_BATCH_SIZE, 1])\n",
    "\n",
    "                # Q-value for next state\n",
    "                Q_nVal = sess.run(DQN_target, feed_dict={X_target: batch_s_tn})\n",
    "\n",
    "                # Choose max of Q-value at state t+1\n",
    "                Q_tn_max = np.amax(Q_nVal, axis=1)\n",
    "\n",
    "                # One-hot encoding of actions chosen\n",
    "                batch_actions = np.zeros([C_BATCH_SIZE, 2])\n",
    "                batch_actions[np.arange(C_BATCH_SIZE), batch_action] = 1\n",
    "\n",
    "                # If the episode has ended, then the total future reward should be zero Q(s_t+1) = 0\n",
    "                is_terminal_state = np.ones([C_BATCH_SIZE, 1])\n",
    "                is_terminal_state[np.where(batch_reward == -1)] = 0\n",
    "\n",
    "                batch_target_val = batch_reward + np.multiply(is_terminal_state,\n",
    "                                                              GAMMA * np.transpose(\n",
    "                                                                  np.array([Q_tn_max])))\n",
    "\n",
    "                _, c_loss = sess.run([trainer, loss], feed_dict={Q_target: batch_target_val,\n",
    "                                                                 X_main: batch_s_t,\n",
    "                                                                 actions: batch_actions,\n",
    "                                                                 batch_size: C_BATCH_SIZE})\n",
    "\n",
    "                total_steps += 1\n",
    "                state = np.copy(n_state)\n",
    "\n",
    "                if is_done:\n",
    "                    break\n",
    "\n",
    "            if (episode + 1) % 5 == 0:\n",
    "                sess.run(updateOp)\n",
    "\n",
    "            # Log the loss and reward to CSV file\n",
    "            data = [episode + 1, total_steps, c_loss[0]]\n",
    "            # helpers.log(data, csv_loss_file)\n",
    "\n",
    "            if (episode + 1) % 5 == 0:\n",
    "                print(data)\n",
    "\n",
    "            # Evaluate performance and log to CSV file\n",
    "            if (episode + 1) % 20 == 0:\n",
    "                data = [episode + 1, total_steps, c_loss[0]] + ['%.4f' % elem for elem in\n",
    "                                                                list(agent_test.test(X_main,\n",
    "                                                                                  DQN_main,\n",
    "                                                                                  sess,\n",
    "                                                                                  NUM_TEST_RUNS))]\n",
    "#                 helpers.log(data, csv_eval_file)\n",
    "                print('Evaluation:')\n",
    "                print(data)\n",
    "\n",
    "            if (episode + 1) % 500 == 0:\n",
    "                saver.save_model(sess, model_filename)\n",
    "\n",
    "        saver.save_model(sess, model_filename)\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    print('Training complete!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env):\n",
    "\n",
    "    # ==============================================================================================\n",
    "    # Initialise Log writer\n",
    "    # ==============================================================================================\n",
    "    global model_filename\n",
    "    global csv_loss_file, csv_eval_file\n",
    "\n",
    "    t = time.time()\n",
    "    ts = datetime.datetime.fromtimestamp(t).strftime('%Y-%m-%d--%H%M-%S')\n",
    "    csv_loss_filename = LOG_FOLDER + 'target_network_q_learning' + '_' + 'loss' + '_' + ts + '.csv'\n",
    "    csv_eval_filename = LOG_FOLDER + 'target_network_q_learning' + '_' + 'eval' + '_' + ts + '.csv'\n",
    "\n",
    "    model_filename = 'target_network_q_learning' + '_' + ts\n",
    "\n",
    "    csv_loss_header = ['episode', 'total_steps', 'loss']\n",
    "    csv_eval_header = ['episode', 'total_steps', 'loss', 'reward_mean', 'reward_stddev',\n",
    "                       'episode_length_mean', 'episode_length_stddev']\n",
    "\n",
    "    with open(csv_loss_filename, 'w') as csv_loss_file, \\\n",
    "            open(csv_eval_filename, 'w') as csv_eval_file:\n",
    "        # Write meta-data and headers to CSV file\n",
    "        csv_writer = csv.writer(csv_loss_file)\n",
    "        csv_writer.writerow(csv_loss_header)\n",
    "\n",
    "        csv_writer = csv.writer(csv_eval_file)\n",
    "        csv_writer.writerow(csv_eval_header)\n",
    "\n",
    "        for experiment in range(NUM_EXPERIMENTS):\n",
    "            print('Experiment number: ', experiment)\n",
    "            target_net_q_learning(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    filename = MODEL_FOLDER + LOAD_MODEL_FILENAME\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        X, _, DQN = build_graph()\n",
    "        tf.global_variables_initializer().run()\n",
    "        saver.load_model(sess, filename)\n",
    "        agent_test.test(X, DQN, sess, 10, render=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
